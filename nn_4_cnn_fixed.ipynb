{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of nn_4_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danipapis/AIweek---DL-workshop/blob/master/nn_4_cnn_fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjNfQQezBBtl",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eyaler/workshop/blob/master/nn_4_cnn.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VPL9z9UsfEgd"
      },
      "source": [
        "# 4. Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jaJB3RfDdoPl",
        "outputId": "da93f992-f9a8-453b-ab01-39ac8ecc10ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from keras import Input, Model\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.datasets import mnist, cifar10\n",
        "from keras.utils import to_categorical\n",
        "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wq3kt-CcdoPu",
        "colab": {}
      },
      "source": [
        "# set random seeds for more reproducible results\n",
        "from numpy.random import seed\n",
        "seed(42)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(43)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OFIQLJfAdoP4",
        "outputId": "397ecb07-019c-4fa7-fe1c-147a9f3c82b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load dataset\n",
        "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "#x_train = np.mean(x_train, axis=-1, keepdims=True) #will turn the input to 1D\n",
        "#x_test = np.mean(x_test, axis=-1, keepdims=True)\n",
        "print(x_train.shape, y_train.shape)\n",
        "\n",
        "train_size = len(y_train)\n",
        "test_size = len(y_test)\n",
        "xdim = x_train.shape[1]\n",
        "ydim = x_train.shape[2]\n",
        "\n",
        "print(x_train.dtype, y_train.dtype)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xOR0rcAvdoQD",
        "colab": {}
      },
      "source": [
        "print(x_train[0])\n",
        "print(y_train[0])\n",
        "if x_train.shape[-1]==1:\n",
        "  plt.imshow(x_train[20002][...,0], cmap='gray')\n",
        "else:\n",
        "  plt.imshow(x_train[20002])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aByrOryzdoQL",
        "colab": {}
      },
      "source": [
        "print(np.min(x_train), np.max(x_train), np.median(x_train))\n",
        "print(np.unique(y_train, return_counts=True))\n",
        "print(np.unique(y_test, return_counts=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3OSVDe_sdoQR",
        "colab": {}
      },
      "source": [
        "n_classes = len(np.unique(y_test))\n",
        "x_train, y_train = shuffle(x_train, y_train, random_state=44)\n",
        "x_train = x_train/255\n",
        "x_test = x_test/255\n",
        "y_train = to_categorical(y_train, n_classes)\n",
        "y_test = to_categorical(y_test, n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mgy7JjGgdoQd",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "    rate = 0.5\n",
        "    # EX: your code here\n",
        "    inputs = Input(shape=(32,32,3)) #input size\n",
        "    f = Conv2D(filters=32, kernel_size=3, activation='relu')(inputs)\n",
        "    f = Conv2D(filters=32, kernel_size=3, activation='relu')(f)\n",
        "    f = MaxPooling2D()(f)\n",
        "    f = Dropout(rate)(f)\n",
        "    f = Conv2D(filters=32, kernel_size=3, activation='relu')(f)\n",
        "    f = Conv2D(filters=32, kernel_size=3, activation='relu')(f)\n",
        "    f = MaxPooling2D()(f)\n",
        "    f = Dropout(rate)(f)\n",
        "    f = Flatten()(f)\n",
        "    f = Dense(512, activation='relu')(f)\n",
        "    f = Dropout(rate)(f)\n",
        "    f = Dense(100, activation='relu')(f)\n",
        "    outputs = Dense(n_classes, activation='softmax')(f)\n",
        "    return Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "181nr_Sq9YUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 80\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3JaZWamydoQk",
        "colab": {}
      },
      "source": [
        "model = get_model()\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JTpa8BPedoQ5",
        "colab": {}
      },
      "source": [
        "# Augmentations\n",
        "\n",
        "model = get_model()\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "datagen = ImageDataGenerator()\n",
        "\n",
        "valid_idx = int(train_size*0.8)\n",
        "aug_train_data = datagen.flow(x=x_train[:valid_idx], y=y_train[:valid_idx], batch_size=batch_size, seed=45)\n",
        "valid_data = (x_train[valid_idx:], y_train[valid_idx:])\n",
        "history = model.fit_generator(aug_train_data, epochs=epochs, steps_per_epoch=valid_idx // batch_size, validation_data=valid_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tzwkn83-doRA",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UHNWmqPZdoRH",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "loss, acc = model.evaluate(x=x_test, y=y_test)\n",
        "print(loss, acc)\n",
        "targets = np.argmax(y_test, axis=-1)\n",
        "probabilities = model.predict(x=x_test)\n",
        "predictions = np.argmax(probabilities, axis=-1)\n",
        "cm = confusion_matrix(y_true=targets, y_pred=predictions)\n",
        "print(cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wpczivZJdoRN",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        title = 'Normalized confusion matrix'\n",
        "    else:\n",
        "        title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    np.set_printoptions(precision=2)\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "classes = np.arange(n_classes)\n",
        "plot_confusion_matrix(cm, classes=classes)\n",
        "plot_confusion_matrix(cm, classes=classes, normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-BGouqTQdoRT",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "report = classification_report(y_true=targets, y_pred=predictions, labels=classes, target_names=class_names)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cYlQPvOWdoRd",
        "colab": {}
      },
      "source": [
        "# top errors\n",
        "from sklearn.metrics import log_loss\n",
        "max_probs = np.max(probabilities, axis=-1)\n",
        "losses = [log_loss(y_true=y, y_pred=prob, eps=1e-7) for y,prob in zip(y_test,probabilities)]\n",
        "print('loss\\tindex\\ttrue\\t\\tpredicted\\tprobability')\n",
        "top_errors = sorted(list(zip(losses, np.arange(test_size), [class_names[t] for t in targets], [class_names[p] for p in predictions], max_probs)), reverse=True)[:10]\n",
        "for error in top_errors:\n",
        "    print('%.04f\\t%d\\t%s\\t\\t%s\\t\\t%.04f'%error)\n",
        "    plt.figure()\n",
        "    if x_test.shape[-1]==1:\n",
        "      plt.imshow(x_test[error[1]][...,0], cmap='gray')\n",
        "    else:\n",
        "      plt.imshow(x_test[error[1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6lJpKFx5ZRF",
        "colab_type": "text"
      },
      "source": [
        "# EX:\n",
        "# 1. do CIFAR-10 (gray) with a CNN:\n",
        "# a. start with the cifar solution to ex2\n",
        "# b. rewrite the model to use convoltions and max-pooling\n",
        "# c. don't optimize too much, just try to get a smaller validation error than ex2, with less degrees of freedom (see model.summary)\n",
        "# 2. do CIFAR-10 (rgb) with a CNN:\n",
        "# a. rewrite the code to deal with rgb channel\n",
        "# b. use an architecture of the form: conv-conv-maxpool-dropout-conv-conv-maxpool-dropout-flatten-dense-dropout-dense+softmax\n",
        "# c. look at the data, overfit, generalize (strive to validation accuracy>0.8), evaluate\n",
        "# d. calculate the number of degrees of freedom by hand (remember the biases), compare to model.summary(), show your calculation\n",
        "# e. add batch normalization after each conv-relu. does it help? was it worthwhile in terms of additional degrees of freedom and training time?\n",
        "# g. add horizontal flips, and small horizontal and vertical shifts random data augmentation. can you get better results?\n",
        "# h. bonus: there is an ongoing debate whether to put batch-norm before or after the relu. compare (you will need seperate the relu from the conv)"
      ]
    }
  ]
}